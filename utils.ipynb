{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516f973e-0761-4545-8074-cf172d75cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import ceil, floor\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "867a33d1-ad0f-47dc-ab51-8a5b9143935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_reduncer_preflight_check(\n",
    "    rankings_a, rankings_b, batch_size=3, iou_threshold=0.3\n",
    "):\n",
    "    sa, sb = set(rankings_a[:batch_size]), set(rankings_b[:batch_size])\n",
    "    iou = len(sa & sb) / len(sa | sb)\n",
    "    if iou > iou_threshold:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e14e8355-0e2c-438c-b3d3-6d2e2642b868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_options(options):\n",
    "    option_strings = ['{}. {}'.format(k, options[k]) for k in options]\n",
    "    return (', '.join(option_strings[:-1]) \n",
    "            + ' or ' + option_strings[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61344628-5401-4a0f-b2e9-9322b8d70235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_prompt_mixtral(context, question, options):\n",
    "    return (\n",
    "        '<s> [INST] '\n",
    "        'You are a helpful medical expert, and your task is '\n",
    "        'to answer a multi-choice medical question using the '\n",
    "        'relevant documents.\\n'\n",
    "        f'{question}'\n",
    "        '\\n'\n",
    "        'Here are the relevant documents:\\n'\n",
    "        f'{context}'\n",
    "        f' [/INST] Please select the answer from {options}.'\n",
    "        ' Also explain the answer briefly in one sentence. </s>'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "262b2d45-2455-4b08-8783-7bfb74e83ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_map_prompt_mixtral(context, question, options):\n",
    "    return (\n",
    "        '<s> [INST] '\n",
    "        'You are a helpful medical expert, and your task is '\n",
    "        'to answer a multi-choice medical question using the '\n",
    "        'relevant documents.\\n'\n",
    "        f'{question}'\n",
    "        '\\n'\n",
    "        'Here are the relevant documents:\\n'\n",
    "        f'{context}'\n",
    "        '\\n'\n",
    "        f' [/INST] Please select the answer from {options},'\n",
    "        ' explain the answer briefly in one sentence,'\n",
    "        ' and format output in JSON.'\n",
    "        ' </s>'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b1f5b9-d93e-4781-9e10-8742d8cbc7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_mixtral_zs(question, options):\n",
    "    return (\n",
    "        '<s> [INST] '\n",
    "        'You are a helpful medical expert, and your task is '\n",
    "        'to answer a multi-choice medical question.\\n'\n",
    "        f'{question}'\n",
    "        f' [/INST] Please select the answer from {options}.'\n",
    "        ' Also explain the answer briefly in one sentence. </s>'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c44af532-2909-4d13-8b0f-e2c6164e399d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_prompt_llama2(context, question, options):\n",
    "    return (\n",
    "        '<s> [INST] <<SYS>>\\n'\n",
    "        'You are a helpful medical expert, and your task is '\n",
    "        'to answer a multi-choice medical question using the '\n",
    "        'relevant documents.\\n<</SYS>>\\n\\n'\n",
    "        f'{question}'\n",
    "        '\\n'\n",
    "        'Here are the relevant documents:\\n'\n",
    "        f'{context}'\n",
    "        f'Please select the answer from {options}.'\n",
    "        ' Also explain the answer briefly in one sentence. [/INST]'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "359e9987-9086-475e-88e4-a3eabf7be0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_map_prompt_llama2(context, question, options):\n",
    "    return (\n",
    "        '<s> [INST] <<SYS>>\\n'\n",
    "        'You are a helpful medical expert, and your task is '\n",
    "        'to answer a multi-choice medical question using the '\n",
    "        'relevant documents.\\n<</SYS>>\\n\\n'\n",
    "        f'{question}'\n",
    "        '\\n'\n",
    "        'Here are the relevant documents:\\n'\n",
    "        f'{context}'\n",
    "        '\\n'\n",
    "        f'Please either select the answer from {options},'\n",
    "        ' explain the answer briefly in one sentence,'\n",
    "        ' and format output in JSON. [/INST]'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37fdd647-c6e7-4a33-840d-e67b2bcaf025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_llama2_zs(question, options):\n",
    "    return (\n",
    "        '<s> [INST] <<SYS>>\\n'\n",
    "        'You are a helpful medical expert, and your task is '\n",
    "        'to answer a multi-choice medical question.\\n<</SYS>>\\n\\n'\n",
    "        f'{question}'\n",
    "        f'Please select the answer from {options}.'\n",
    "        ' Also explain the answer briefly in one sentence. [/INST]'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9416e7c8-d9a8-43be-8fc7-2460fbf77dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_prompt_llama3(context, question, options):\n",
    "    return (\n",
    "        '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n'\n",
    "        'You are a helpful medical expert, and your task is '\n",
    "        'to answer a multi-choice medical question using the relevant documents.'\n",
    "        '<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n'\n",
    "        f'{question}'\n",
    "        '\\n'\n",
    "        'Here are the relevant documents:\\n'\n",
    "        f'{context}'\n",
    "        f'Please select the answer from {options}.'\n",
    "        ' Also explain the answer briefly in one sentence. '\n",
    "        '<|eot_id|>'\n",
    "        '<|start_header_id|>assistant<|end_header_id|>'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40004e5b-0244-4fdb-932d-3004fdcda08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_map_prompt_llama3(context, question, options):\n",
    "    return (\n",
    "        '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n'\n",
    "        'You are a helpful medical expert, and your task is '\n",
    "        'to answer a multi-choice medical question using the '\n",
    "        'relevant documents.'\n",
    "        '<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n'\n",
    "        f'{question}'\n",
    "        '\\n'\n",
    "        'Here are the relevant documents:\\n'\n",
    "        f'{context}'\n",
    "        '\\n'\n",
    "        f'Please select the answer from {options}.'\n",
    "        ' Also explain the answer briefly in one sentence. '\n",
    "        '<|eot_id|>'\n",
    "        '<|start_header_id|>assistant<|end_header_id|>'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76b6b0c9-e1b0-40ea-af45-4446c0814dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_llama3_zs(question, options):\n",
    "    return (\n",
    "        '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n'\n",
    "        'You are a helpful medical expert, and your task is '\n",
    "        'to answer a multi-choice medical question.'\n",
    "        '<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n'\n",
    "        f'{question}'\n",
    "        f'Please select the answer from {options}.'\n",
    "        ' Also explain the answer briefly in one sentence. '\n",
    "        '<|eot_id|>'\n",
    "        '<|start_header_id|>assistant<|end_header_id|>'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2e5bcf8-7298-48a9-b9ac-0e616dedd65a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_prompt_gpt35(context, question, options):\n",
    "    return json.dumps([\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': (\n",
    "                'You are a helpful medical expert, and your task is '\n",
    "                'to answer a multi-choice medical question using the '\n",
    "                'relevant documents.'\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': (\n",
    "                f'{question}\\n'\n",
    "                'Here are the relevant documents:\\n'\n",
    "                f'{context}\\n'\n",
    "                f'Please select the answer from {options},'\n",
    "                ' explain the answer briefly in one sentence,'\n",
    "                ' and format output in JSON.'\n",
    "            ),\n",
    "        },\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22933c4a-1b15-4916-b6bc-e043700b09fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_map_prompt_gpt35(context, question, options):\n",
    "    return json.dumps([\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': (\n",
    "                'You are a helpful medical expert, and your task is '\n",
    "                'to answer a multi-choice medical question using the '\n",
    "                'relevant documents.'\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': (\n",
    "                f'{question}\\n'\n",
    "                'Here are the relevant documents:\\n'\n",
    "                f'{context}\\n'\n",
    "                f'Please select the answer from {options},'\n",
    "                ' explain the answer briefly in one sentence,'\n",
    "                ' and format output in JSON.'\n",
    "            ),\n",
    "        },\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bd10941-8335-46f1-9748-d70d6ccab894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_gpt35_zs(question, options):\n",
    "    return json.dumps([\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': (\n",
    "                'You are a helpful medical expert, and your task is '\n",
    "                'to answer a multi-choice medical question using the '\n",
    "                'relevant documents.'\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': (\n",
    "                f'{question}\\n'\n",
    "                f'Please select the answer from {options},'\n",
    "                ' explain the answer briefly in one sentence,'\n",
    "                ' and format output in JSON.'\n",
    "            ),\n",
    "        },\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1f5e08d-ee03-4dc5-9e3c-8792642de7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_prompt(\n",
    "    context, \n",
    "    question, \n",
    "    options, \n",
    "    model_id,\n",
    "):\n",
    "    if 'gpt' in model_id:\n",
    "        if '35' in model_id:\n",
    "            return format_prompt_gpt35(\n",
    "                context, question, options)\n",
    "    if 'llama2' in model_id:\n",
    "        return format_prompt_llama2(\n",
    "            context, question, options)\n",
    "    if 'llama3' in model_id:\n",
    "        return format_prompt_llama3(\n",
    "            context, question, options)\n",
    "    if 'mixtral' in model_id:\n",
    "        return format_prompt_mixtral(\n",
    "            context, question, options)\n",
    "    return 'Invalid model ID.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "410e8481-43c4-4277-b07c-1d31983da835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_map_prompt(\n",
    "    context, \n",
    "    question, \n",
    "    options, \n",
    "    model_id,\n",
    "):\n",
    "    if 'gpt' in model_id:\n",
    "        if '35' in model_id:\n",
    "            return format_context_map_prompt_gpt35(\n",
    "                context, question, options)\n",
    "    if 'llama2' in model_id:\n",
    "        return format_context_map_prompt_llama2(\n",
    "            context, question, options)\n",
    "    if 'llama3' in model_id:\n",
    "        return format_context_map_prompt_llama3(\n",
    "            context, question, options)\n",
    "    if 'mixtral' in model_id:\n",
    "        return format_context_map_prompt_mixtral(\n",
    "            context, question, options)\n",
    "    return 'Invalid model ID.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "189376fb-8131-40d2-8304-50f3287f4adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_zs(\n",
    "    question, \n",
    "    options, \n",
    "    model_id,\n",
    "):\n",
    "    if 'gpt' in model_id:\n",
    "        if '35' in model_id:\n",
    "            return format_prompt_gpt35_zs(\n",
    "                question, options)\n",
    "    if 'llama2' in model_id:\n",
    "        return format_prompt_llama2_zs(\n",
    "            question, options)\n",
    "    if 'llama3' in model_id:\n",
    "        return format_prompt_llama3_zs(\n",
    "            question, options)\n",
    "    if 'mixtral' in model_id:\n",
    "        return format_prompt_mixtral_zs(\n",
    "            question, options)\n",
    "    return 'Invalid model ID.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e105ed1-ae16-4586-a4bf-2b221a345704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bedrock_response_titan(prompt, model_id, client):\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "    parameters = {\n",
    "        \"maxTokenCount\":8192,\n",
    "        \"stopSequences\":[],\n",
    "        \"temperature\":0,\n",
    "        \"topP\":1\n",
    "    }\n",
    "    body = json.dumps({\n",
    "        \"inputText\": prompt, \n",
    "        \"textGenerationConfig\": parameters,\n",
    "    })\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            body=body, \n",
    "            modelId=model_id, \n",
    "            accept=accept, \n",
    "            contentType=contentType\n",
    "        )\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        completion = response_body.get('results')[0].get('outputText')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        completion = 'The model is not able to process the request.'\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b7b3436-1003-4e4a-9474-90073317bfe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bedrock_response_mixtral(prompt, model_id, client):\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "    body = json.dumps({\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 512,\n",
    "        \"top_p\": 0.8,\n",
    "        \"temperature\": 0.2,\n",
    "    })\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            body=body, \n",
    "            modelId=model_id, \n",
    "            accept=accept, \n",
    "            contentType=contentType\n",
    "        )\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        completion = response_body.get('outputs')[0].get('text').strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        completion = 'The model is not able to process the request.'\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce32ace7-5605-484d-99d7-ae203114add9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bedrock_response_llama2(prompt, model_id, client):\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "    body = json.dumps({\n",
    "        \"prompt\": prompt,\n",
    "        \"max_gen_len\": 512,\n",
    "        \"temperature\": 0.2,\n",
    "    })\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            body=body, \n",
    "            modelId=model_id, \n",
    "            accept=accept, \n",
    "            contentType=contentType\n",
    "        )\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        completion = response_body.get('generation').strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        completion = 'The model is not able to process the request.'\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d32b3ed2-485f-4ea5-b13b-97ecba3366b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bedrock_response_llama3(prompt, model_id, client):\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "    body = json.dumps({\n",
    "        \"prompt\": prompt,\n",
    "        \"max_gen_len\": 512,\n",
    "        \"temperature\": 0.2,\n",
    "    })\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            body=body, \n",
    "            modelId=model_id, \n",
    "            accept=accept, \n",
    "            contentType=contentType\n",
    "        )\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        completion = response_body.get('generation').strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        completion = 'The model is not able to process the request.'\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cd2c760-fbd2-41f1-b375-94b04bf93579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_response_bedrock(prompt, model_id, client):\n",
    "    if 'titan' in model_id:\n",
    "        return get_bedrock_response_titan(prompt, model_id, client)\n",
    "    if 'mixtral' in model_id:\n",
    "        return get_bedrock_response_mixtral(prompt, model_id, client)\n",
    "    if 'llama2' in model_id:\n",
    "        return get_bedrock_response_llama2(prompt, model_id, client)\n",
    "    if 'llama3' in model_id:\n",
    "        return get_bedrock_response_llama3(prompt, model_id, client)\n",
    "    return 'Invalid model ID.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60a9a551-4e0b-4ba1-861c-e4c13cdd1f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_response_openai(prompt, model_id, client):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model = 'gpt-3.5-turbo-0125',\n",
    "            response_format = { \"type\": \"json_object\" },\n",
    "            messages = json.loads(prompt),\n",
    "            temperature=0.2,\n",
    "            top_p=0.1,\n",
    "            seed=0\n",
    "        )\n",
    "        completion = response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        completion = 'The model is not able to process the request.'\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c61603b-86bb-4152-bb16-ee7f8ee76b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_response(prompt, model_id, model_runtime):\n",
    "    if ('titan' in model_id \n",
    "        or 'mixtral' in model_id \n",
    "        or 'llama' in model_id\n",
    "       ):\n",
    "        return get_model_response_bedrock(\n",
    "            prompt, model_id, model_runtime)\n",
    "    if 'gpt' in model_id:\n",
    "        return get_model_response_openai(\n",
    "            prompt, model_id, model_runtime\n",
    "        )\n",
    "    return 'Invalid model_id.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2856c05f-de06-4ba2-a207-3c2a6f5abf13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_context_map_questions(\n",
    "    qid2query,\n",
    "    qid2answer,\n",
    "    qid2retrieval,\n",
    "    qid,\n",
    "    docs_content,\n",
    "    model_id,\n",
    "    batch_size=4, \n",
    "    top_k=16,\n",
    "):\n",
    "    requests = []\n",
    "    question = qid2query[qid]\n",
    "    if 'options' in qid2answer[qid]: \n",
    "        options = format_options(qid2answer[qid]['options'])\n",
    "    doc_ids = qid2retrieval[qid]['doc_ids']\n",
    "    snippets = [\n",
    "        docs_content[sid] \n",
    "        for sid in doc_ids\n",
    "        if sid in docs_content\n",
    "    ]\n",
    "\n",
    "    index = 0\n",
    "    while index < top_k:\n",
    "        context = '\\n'.join(snippets[:top_k][index:index+batch_size])\n",
    "        prompt = format_context_map_prompt(context, question, options, model_id)\n",
    "        requests.append(prompt)\n",
    "        index += batch_size\n",
    "    return requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "870bb3de-233c-42fd-abaa-239f5cc583be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_context_reduce_prompt_mixtral(\n",
    "    qid2query,\n",
    "    qid2answer,\n",
    "    qid2retrieval,\n",
    "    qid,\n",
    "    responses\n",
    "):\n",
    "    question = qid2query[qid]\n",
    "    if 'options' in qid2answer[qid]: \n",
    "        options = format_options(qid2answer[qid]['options'])\n",
    "    filtered_responses = [\n",
    "        r for r in responses \n",
    "        if 'no information detected' not in r.lower()]\n",
    "    context = '\\n\\n'.join([f'{r}' for r in filtered_responses])\n",
    "    \n",
    "    return (\n",
    "        '<s> [INST] '\n",
    "        'You are a helpful medical expert, and your task is '\n",
    "        'to answer a multi-choice medical question using the '\n",
    "        'information extracted from relevant documents.\\n'\n",
    "        f'{question}'\n",
    "        '\\n'\n",
    "        'Here is the extracted information:\\n'\n",
    "        f'{context}'\n",
    "        '\\n'\n",
    "        f'{question}'\n",
    "        f' [/INST] Please ignore answers that do not contain information'\n",
    "        f' and select the answer from {options}. </s>'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36c8787a-bda0-4059-a1d6-28923dc507e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_context_reduce_prompt_llama2(\n",
    "    qid2query,\n",
    "    qid2answer,\n",
    "    qid2retrieval,\n",
    "    qid,\n",
    "    responses\n",
    "):\n",
    "    question = qid2query[qid]\n",
    "    if 'options' in qid2answer[qid]: \n",
    "        options = format_options(qid2answer[qid]['options'])\n",
    "    filtered_responses = [\n",
    "        r for r in responses \n",
    "        if 'no information detected' not in r.lower()]\n",
    "    context = '\\n\\n'.join([f'{r}' for r in filtered_responses])\n",
    "    \n",
    "    return (\n",
    "        '<s> [INST] <<SYS>>\\n'\n",
    "        'You are a helpful medical expert, and your task is '\n",
    "        'to answer a multi-choice medical question using the '\n",
    "        'information extracted from relevant documents.\\n<</SYS>>\\n\\n'\n",
    "        f'{question}'\n",
    "        '\\n'\n",
    "        'Here is the extracted information:\\n'\n",
    "        f'{context}'\n",
    "        '\\n'\n",
    "        f'{question}'\n",
    "        f' [/INST] Please ignore answers that do not contain information'\n",
    "        f' and select the answer from {options}. [/INST]'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce44e8da-b33d-4cab-8801-9d300d590161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_context_reduce_prompt_llama3(\n",
    "    qid2query,\n",
    "    qid2answer,\n",
    "    qid2retrieval,\n",
    "    qid,\n",
    "    responses\n",
    "):\n",
    "    question = qid2query[qid]\n",
    "    if 'options' in qid2answer[qid]: \n",
    "        options = format_options(qid2answer[qid]['options'])\n",
    "    filtered_responses = [\n",
    "        r for r in responses \n",
    "        if 'no information detected' not in r.lower()]\n",
    "    context = '\\n\\n'.join([f'{r}' for r in filtered_responses])\n",
    "\n",
    "    return (\n",
    "        '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n'\n",
    "        'You are a helpful medical expert, and your task is '\n",
    "        'to answer a multi-choice medical question using the relevant documents.'\n",
    "        '<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n'\n",
    "        f'{question}'\n",
    "        '\\n'\n",
    "        'Here is the extracted information:\\n'\n",
    "        f'{context}'\n",
    "        f' [/INST] Please ignore answers that do not contain information'\n",
    "        f' and select the answer from {options}.'\n",
    "        '<|eot_id|><|start_header_id|>assistant<|end_header_id|>'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c438d5a-0d08-41f8-9711-77a91ed8b82c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_context_reduce_prompt_gpt35(\n",
    "    qid2query,\n",
    "    qid2answer,\n",
    "    qid2retrieval,\n",
    "    qid,\n",
    "    responses\n",
    "):\n",
    "    question = qid2query[qid]\n",
    "    if 'options' in qid2answer[qid]: \n",
    "        options = format_options(qid2answer[qid]['options'])\n",
    "    filtered_responses = [\n",
    "        r for r in responses \n",
    "        if 'no information detected' not in r.lower()]\n",
    "    context = '\\n\\n'.join([f'{r}' for r in filtered_responses])\n",
    "\n",
    "    return json.dumps([\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': (\n",
    "                'You are a helpful medical expert, and your task is '\n",
    "                'to answer a multi-choice medical question using the '\n",
    "                'relevant documents.'\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': (\n",
    "                f'{question}\\n'\n",
    "                'Here is the extracted information:\\n'\n",
    "                f'{context}\\n'\n",
    "                f'{question}\\n'\n",
    "                'Please ignore answers that do not contain information,'\n",
    "                f' select the answer from {options},'\n",
    "                ' and format the output in JSON.'\n",
    "            ),\n",
    "        },\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04a33030-59a7-4690-b5f8-e4c3e83c364c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_context_reduce_prompt(\n",
    "    qid2query,\n",
    "    qid2answer,\n",
    "    qid2retrieval,\n",
    "    qid,\n",
    "    responses,\n",
    "    model_id\n",
    "):\n",
    "    if 'mixtral' in model_id:\n",
    "        return format_context_reduce_prompt_mixtral(\n",
    "            qid2query,\n",
    "            qid2answer,\n",
    "            qid2retrieval,\n",
    "            qid,\n",
    "            responses\n",
    "        )\n",
    "    if 'llama2' in model_id:\n",
    "        return format_context_reduce_prompt_llama2(\n",
    "            qid2query,\n",
    "            qid2answer,\n",
    "            qid2retrieval,\n",
    "            qid,\n",
    "            responses\n",
    "        )\n",
    "    if 'llama3' in model_id:\n",
    "        return format_context_reduce_prompt_llama3(\n",
    "            qid2query,\n",
    "            qid2answer,\n",
    "            qid2retrieval,\n",
    "            qid,\n",
    "            responses\n",
    "        )\n",
    "    if 'gpt35' in model_id:\n",
    "        return format_context_reduce_prompt_gpt35(\n",
    "            qid2query,\n",
    "            qid2answer,\n",
    "            qid2retrieval,\n",
    "            qid,\n",
    "            responses\n",
    "        )\n",
    "    return 'Invalid model ID in context reduce prompt.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "349c98cb-c824-4070-81d0-aeb8fe5756b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def brief_context_main(\n",
    "    qid2query,\n",
    "    qid2answer,\n",
    "    qid2retrieval,\n",
    "    qid,\n",
    "    docs_content,\n",
    "    batch_size, \n",
    "    top_k,\n",
    "    model_id,\n",
    "    model_runtime\n",
    "):\n",
    "    answer = {}\n",
    "    questions = format_context_map_questions(\n",
    "        qid2query,\n",
    "        qid2answer,\n",
    "        qid2retrieval,\n",
    "        qid,\n",
    "        docs_content,\n",
    "        model_id,\n",
    "        batch_size=batch_size,\n",
    "        top_k=top_k)\n",
    "    answers = []\n",
    "    for question in questions:\n",
    "        answers.append(\n",
    "            get_model_response(\n",
    "                question, model_id, model_runtime\n",
    "            ).lstrip().rstrip())\n",
    "        time.sleep(0.1)\n",
    "    summary_request = format_context_reduce_prompt(\n",
    "        qid2query,\n",
    "        qid2answer,\n",
    "        qid2retrieval, \n",
    "        qid, \n",
    "        answers, \n",
    "        model_id\n",
    "    )\n",
    "    final_answer = get_model_response(\n",
    "        summary_request, model_id, model_runtime)\n",
    "    answer['answer_from_each_batch'] = answers\n",
    "    answer['final_answer'] = final_answer\n",
    "    time.sleep(0.1)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80588969-2539-4e13-891c-3aa612ef33d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_answer_positions(\n",
    "    position_range, \n",
    "    exact_position,\n",
    "    start,\n",
    "    end,\n",
    "    top_k,\n",
    "):\n",
    "    if position_range == 'middle':\n",
    "        position_candidates = [\n",
    "            i for i in range(floor(top_k*0.25), ceil(top_k*0.75))]\n",
    "    elif position_range == 'top':\n",
    "        position_candidates = [i for i in range(0, ceil(top_k*0.25))]\n",
    "    elif position_range == 'bottom':\n",
    "        position_candidates = [i for i in range(floor(top_k*0.75), top_k)]\n",
    "    elif 0 <= exact_position and exact_position < top_k:\n",
    "        position_candidates = [exact_position]\n",
    "    else:\n",
    "        position_candidates = [i for i in range(start, end)]\n",
    "    return position_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed215c1d-b421-4d45-95ab-c9925850bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_question_ids(all_qids, qid2answer, datasets, size=35):\n",
    "    selected_qids = []\n",
    "    answer_choices = {\n",
    "        'pubmedqa': ['A', 'B', 'C'],\n",
    "        'bioasq': ['A', 'B'],\n",
    "    }\n",
    "    for dataset in datasets: # ['pubmedqa']:\n",
    "        for answer in answer_choices[dataset]:\n",
    "            selected_qids += random.sample([\n",
    "                q for q in all_qids if (\n",
    "                    q.startswith(dataset) \n",
    "                    and qid2answer[q]['answer']==answer\n",
    "                )], size)\n",
    "        # selected_qids += [q for q in all_qids if q.startswith(dataset)]\n",
    "    return selected_qids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc46056e-d6ad-4d35-83a2-8ebb84518eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_synthetic_rankings(\n",
    "    answer_positions, \n",
    "    key_ids, \n",
    "    non_key_ids,\n",
    "    top_k,\n",
    "):\n",
    "    synthetic_ids = []\n",
    "    index_answers, index_random_samples = 0, 0\n",
    "    for i in range(top_k):\n",
    "        if i in answer_positions:\n",
    "            synthetic_ids.append(key_ids[index_answers])\n",
    "            index_answers += 1\n",
    "        else:\n",
    "            synthetic_ids.append(\n",
    "                non_key_ids[index_random_samples])\n",
    "            index_random_samples += 1\n",
    "    return synthetic_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2173f2c-f1cf-40d8-b6cf-6c163ec7f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_dataset_with_synthetic_ranking(\n",
    "    qid2retrieval,\n",
    "    qid2query,\n",
    "    qid2answer,\n",
    "    qid2key_info,\n",
    "    docs_content,\n",
    "    fout,\n",
    "    top_k,\n",
    "    random_seed=0,\n",
    "    position='',\n",
    "    specific_index=-1,\n",
    "    start=-1,\n",
    "    end=-1,\n",
    "    sample_size=24,\n",
    "    datasets=[],\n",
    "):\n",
    "    assert (\n",
    "        position in ['top', 'middle', 'bottom']\n",
    "        or 0 <= specific_index and specific_index < top_k\n",
    "        or 0 <= start and start < end and end <= top_k\n",
    "    )\n",
    "    synthetic_dataset = {}\n",
    "    random.seed(random_seed)\n",
    "    qids = sample_question_ids(\n",
    "        [q for q in qid2retrieval.keys()],\n",
    "        qid2answer,\n",
    "        datasets,\n",
    "        sample_size,\n",
    "    )\n",
    "    for qid in qids:\n",
    "        answer_ids = qid2key_info[qid]\n",
    "        random.shuffle(answer_ids)\n",
    "        n_answer = 1\n",
    "        non_answer_ids = [\n",
    "            sid for sid in qid2retrieval[qid]['doc_ids'] \n",
    "            if sid not in answer_ids\n",
    "        ][:top_k-n_answer]\n",
    "        position_candidates = sample_answer_positions(\n",
    "            position, specific_index, start, end, top_k)   \n",
    "        answer_positions = random.sample(position_candidates, n_answer)\n",
    "        synthetic_ids = construct_synthetic_rankings(\n",
    "            answer_positions, \n",
    "            answer_ids, \n",
    "            non_answer_ids,\n",
    "            top_k,\n",
    "        )\n",
    "        synthetic_dataset[qid] = {\n",
    "            'qid': qid,\n",
    "            'question': qid2query[qid],\n",
    "            'key_info': qid2key_info[qid],\n",
    "            'doc_ids': qid2retrieval[qid]['doc_ids'],\n",
    "            'synthetic_rankings': synthetic_ids,\n",
    "        }\n",
    "    json.dump(\n",
    "        synthetic_dataset,\n",
    "        open(fout, 'w+'),\n",
    "        indent=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be5ef6-6e25-4bd9-84b4-8746bc3bd393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
